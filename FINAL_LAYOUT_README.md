# Final Repository Layout - Post-Migration

**Status**: âœ… **MIGRATION COMPLETE**  
**Date**: 2026-01-19  
**Purpose**: Document the final repository structure after refactoring and provide guidance on using the codebase

---

## ğŸ“‹ Table of Contents

1. [Migration Summary](#migration-summary)
2. [Final Repository Structure](#final-repository-structure)
3. [File Locations Reference](#file-locations-reference)
4. [Import Patterns](#import-patterns)
5. [Running Tools](#running-tools)
6. [Dependency Management](#dependency-management)
7. [Remaining Gaps](#remaining-gaps)
8. [Development Workflow](#development-workflow)

---

## ğŸ‰ Migration Summary

### What Was Accomplished

The repository has been successfully refactored from a flat structure with 52+ root-level Python files to a well-organized package structure:

**Before Migration:**
- 52 Python files in root directory
- Inconsistent import patterns (relative imports, sys.path hacks)
- No clear separation between package code and scripts
- Difficult to install as a Python package

**After Migration:**
- âœ… **41 root-level wrappers** (thin, backwards-compatible)
- âœ… **44 canonical implementations** in `src/municipal_scrape_workspace/`
- âœ… **11 superseded files** archived in `archive/ccindex/superseded/`
- âœ… **Clean package imports** (no sys.path manipulation)
- âœ… **Installable Python package** via pip
- âœ… **Backwards compatible** - old scripts still work

### Statistics

```
Files Migrated:     44 Python files
Wrappers Created:   41 backwards-compatible wrappers
Files Archived:     11 superseded/duplicate files
Import Fixes:       All files now use proper package imports
Documentation:      6 comprehensive guides created
```

---

## ğŸ“ Final Repository Structure

```
municipal_scrape_workspace/
â”‚
â”œâ”€â”€ src/municipal_scrape_workspace/          # â­ Installable Python package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py                               # Main CLI entrypoint
â”‚   â”œâ”€â”€ orchestrate_municipal_scrape.py      # Municipal scrape orchestrator
â”‚   â”œâ”€â”€ check_archive_callbacks.py           # Archive integration checks
â”‚   â”‚
â”‚   â””â”€â”€ ccindex/                             # Common Crawl tooling
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”‚
â”‚       â”œâ”€â”€ # ğŸ”§ Core Pipeline Orchestration (4 files)
â”‚       â”œâ”€â”€ cc_pipeline_orchestrator.py      # Main orchestration
â”‚       â”œâ”€â”€ cc_pipeline_watch.py             # Progress watcher
â”‚       â”œâ”€â”€ cc_pipeline_hud.py               # Status HUD
â”‚       â”œâ”€â”€ monitor_progress.py              # Progress monitoring
â”‚       â”‚
â”‚       â”œâ”€â”€ # ğŸ“¦ Conversion Tools (5 files)
â”‚       â”œâ”€â”€ bulk_convert_gz_to_parquet.py    # Bulk conversion
â”‚       â”œâ”€â”€ parallel_convert_missing.py      # Parallel missing conversions
â”‚       â”œâ”€â”€ regenerate_parquet_from_gz.py    # Regenerate from GZ
â”‚       â”œâ”€â”€ sample_ccindex_to_parquet.py     # Sample conversion
â”‚       â”œâ”€â”€ extract_cc_index_tarballs.py     # Tarball extraction
â”‚       â”‚
â”‚       â”œâ”€â”€ # ğŸ“Š Sorting Tools (2 files)
â”‚       â”œâ”€â”€ sort_cc_parquet_shards.py        # Standard sorting
â”‚       â”œâ”€â”€ sort_unsorted_memory_aware.py    # Memory-aware sorting (canonical)
â”‚       â”‚
â”‚       â”œâ”€â”€ # âœ… Validation Tools (5 files)
â”‚       â”œâ”€â”€ validate_and_sort_parquet.py     # Validate & sort
â”‚       â”œâ”€â”€ parallel_validate_parquet.py     # Parallel validation
â”‚       â”œâ”€â”€ validate_urlindex_sorted.py      # Check sort order
â”‚       â”œâ”€â”€ validate_search_completeness.py  # Search validation
â”‚       â”œâ”€â”€ validate_collection_completeness.py # Collection validation
â”‚       â”‚
â”‚       â”œâ”€â”€ # ğŸ—ï¸ Index Builders (5 files)
â”‚       â”œâ”€â”€ build_cc_pointer_duckdb.py       # Pointer index builder
â”‚       â”œâ”€â”€ build_index_from_parquet.py      # Generic builder
â”‚       â”œâ”€â”€ build_parallel_duckdb_indexes.py # Parallel builder
â”‚       â”œâ”€â”€ build_duckdb_pointer_from_parquet.py # DuckDB from parquet
â”‚       â”œâ”€â”€ build_cc_parquet_rowgroup_index.py # Rowgroup index
â”‚       â”‚
â”‚       â”œâ”€â”€ # ğŸ“‘ Meta-Index Tools (2 files)
â”‚       â”œâ”€â”€ build_year_meta_indexes.py       # Year-based meta-indexes
â”‚       â”œâ”€â”€ build_master_index.py            # Master index
â”‚       â”‚
â”‚       â”œâ”€â”€ # ğŸ” Search Tools (5 files)
â”‚       â”œâ”€â”€ search_cc_domain.py              # Domain search
â”‚       â”œâ”€â”€ search_cc_duckdb_index.py        # DuckDB search
â”‚       â”œâ”€â”€ search_cc_pointer_index.py       # Pointer search
â”‚       â”œâ”€â”€ search_cc_via_meta_indexes.py    # Meta-index search
â”‚       â”œâ”€â”€ search_parallel_duckdb_indexes.py # Parallel search
â”‚       â”œâ”€â”€ cc_domain_parquet_locator.py     # Domain file locator
â”‚       â”‚
â”‚       â”œâ”€â”€ # ğŸ“¥ WARC Retrieval Tools (4 files)
â”‚       â”œâ”€â”€ download_warc_records.py         # Download WARC records
â”‚       â”œâ”€â”€ verify_warc_retrieval.py         # Verify downloads
â”‚       â”œâ”€â”€ validate_warc_record_blobs.py    # Validate WARC data
â”‚       â”œâ”€â”€ warc_candidates_from_jsonl.py    # Extract candidates
â”‚       â”‚
â”‚       â””â”€â”€ # ğŸ‘ï¸ Monitoring/Queue Tools (6 files)
â”‚           â”œâ”€â”€ queue_cc_pointer_build.py    # Build queue
â”‚           â”œâ”€â”€ launch_cc_pointer_build.py   # Launch builds
â”‚           â”œâ”€â”€ monitor_cc_pointer_build.py  # Monitor builds
â”‚           â”œâ”€â”€ watchdog_cc_pointer_build.py # Build watchdog
â”‚           â”œâ”€â”€ watchdog_monitor.py          # Watchdog monitor
â”‚           â””â”€â”€ cc_pointer_status.py         # Status reporter
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ops/                                 # Operational shell scripts
â”‚       â”œâ”€â”€ download_cc_indexes.sh
â”‚       â”œâ”€â”€ overnight_build_duckdb_index.sh
â”‚       â”œâ”€â”€ monitor_cc_2year_download.sh
â”‚       â”œâ”€â”€ rebuild_with_sorted_ranges.sh
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ ccindex/                             # Performance benchmarks
â”‚       â”œâ”€â”€ benchmark_duckdb_pointer.py
â”‚       â”œâ”€â”€ benchmark_cc_domain_search.py
â”‚       â”œâ”€â”€ benchmark_parallel_duckdb_indexes.py
â”‚       â”œâ”€â”€ benchmark_results.json
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ archive/
â”‚   â””â”€â”€ ccindex/
â”‚       â”œâ”€â”€ converters/                      # One-off conversion scripts
â”‚       â”‚   â”œâ”€â”€ convert_final_three*.py
â”‚       â”‚   â””â”€â”€ convert_missing*.py
â”‚       â”‚
â”‚       â””â”€â”€ superseded/                      # âš ï¸ Deprecated/superseded tools
â”‚           â”œâ”€â”€ README.md                    # Why each was archived
â”‚           â”œâ”€â”€ cc_pipeline_manager.py       # Superseded by orchestrator
â”‚           â”œâ”€â”€ consolidate_parquet_files.py
â”‚           â”œâ”€â”€ sort_unsorted_files.py
â”‚           â”œâ”€â”€ sort_parquet_external_merge.py
â”‚           â”œâ”€â”€ validate_and_mark_sorted.py
â”‚           â”œâ”€â”€ build_duckdb_from_sorted_parquet.py
â”‚           â”œâ”€â”€ compare_crawl_results.py
â”‚           â”œâ”€â”€ search_domain_duckdb_pointer.py
â”‚           â”œâ”€â”€ search_domain_pointer_index.py
â”‚           â”œâ”€â”€ search_duckdb_domain.py
â”‚           â””â”€â”€ search_duckdb_pointer_domain.py
â”‚
â”œâ”€â”€ docs/                                    # Documentation
â”‚   â”œâ”€â”€ REPO_LAYOUT_PLAN.md
â”‚   â”œâ”€â”€ COMMON_CRAWL_USAGE.md
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ <root-level wrappers>                    # ğŸ”„ Backwards compatibility
â”‚   â”œâ”€â”€ search_cc_domain.py                  # All root .py files are now
â”‚   â”œâ”€â”€ build_cc_pointer_duckdb.py           # thin wrappers that import
â”‚   â”œâ”€â”€ cc_pipeline_orchestrator.py          # from src/ and call main()
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ pyproject.toml                           # Package configuration
â”œâ”€â”€ bootstrap.sh                             # Setup script
â”œâ”€â”€ README.md                                # Project overview
â””â”€â”€ FINAL_LAYOUT_README.md                   # This file
```

---

## ğŸ“‹ File Locations Reference

### Quick Lookup Table

| Tool Category | Files | Location |
|--------------|-------|----------|
| **Orchestration** | 4 files | `src/.../ccindex/` |
| **Conversion** | 5 files | `src/.../ccindex/` |
| **Sorting** | 2 files | `src/.../ccindex/` |
| **Validation** | 5 files | `src/.../ccindex/` |
| **Index Building** | 5 files | `src/.../ccindex/` |
| **Meta-Indexes** | 2 files | `src/.../ccindex/` |
| **Search** | 6 files | `src/.../ccindex/` |
| **WARC Retrieval** | 4 files | `src/.../ccindex/` |
| **Monitoring/Queue** | 6 files | `src/.../ccindex/` |
| **Municipal Scrape** | 2 files | `src/municipal_scrape_workspace/` |
| **Shell Scripts** | All | `scripts/ops/` (canonical) |
| **Benchmarks** | 10 files | `benchmarks/ccindex/` |
| **Archived** | 11 files | `archive/ccindex/superseded/` |
| **Root Wrappers** | 41 files | Root (backwards compatibility) |

### Core Tool Locations

**Orchestration & Monitoring:**
```
src/municipal_scrape_workspace/ccindex/
â”œâ”€â”€ cc_pipeline_orchestrator.py
â”œâ”€â”€ cc_pipeline_watch.py
â”œâ”€â”€ cc_pipeline_hud.py
â””â”€â”€ monitor_progress.py
```

**Index Building:**
```
src/municipal_scrape_workspace/ccindex/
â”œâ”€â”€ build_cc_pointer_duckdb.py
â”œâ”€â”€ build_parallel_duckdb_indexes.py
â”œâ”€â”€ build_year_meta_indexes.py
â””â”€â”€ build_master_index.py
```

**Search:**
```
src/municipal_scrape_workspace/ccindex/
â”œâ”€â”€ search_cc_via_meta_indexes.py
â”œâ”€â”€ search_cc_domain.py
â”œâ”€â”€ search_parallel_duckdb_indexes.py
â””â”€â”€ search_cc_pointer_index.py
```

---

## ğŸ”— Import Patterns

### âœ… Correct Import Patterns (Use These!)

#### 1. Importing from ccindex tools

```python
# Import a function/class from a ccindex module
from municipal_scrape_workspace.ccindex.search_cc_domain import search_domain
from municipal_scrape_workspace.ccindex.validate_collection_completeness import CollectionValidator

# Import entire module
from municipal_scrape_workspace.ccindex import search_cc_domain
```

#### 2. Importing from municipal scrape tools

```python
# Import from main package
from municipal_scrape_workspace.orchestrate_municipal_scrape import run_scrape
from municipal_scrape_workspace.check_archive_callbacks import check_callbacks
```

#### 3. Running as modules

```python
# In scripts or other code
import subprocess
subprocess.run([
    "python", "-m", 
    "municipal_scrape_workspace.ccindex.search_cc_domain",
    "--domain", "example.com"
])
```

### âŒ Incorrect Import Patterns (Don't Use These!)

```python
# âŒ DON'T: Relative imports from root
import search_cc_domain  # Won't work from installed package

# âŒ DON'T: sys.path manipulation
import sys
sys.path.insert(0, "/home/user/some/path")
from search_cc_domain import main

# âŒ DON'T: Importing from root wrappers
from search_cc_domain import main  # Imports wrapper, not canonical

# âŒ DON'T: Hardcoded paths
sys.path.insert(0, "/home/barberb/ipfs_datasets_py")
```

### ğŸ”§ Code Example: Before & After Migration

**Before Migration (âŒ):**
```python
#!/usr/bin/env python3
import sys
from pathlib import Path

# Bad: sys.path hack
sys.path.insert(0, str(Path(__file__).parent))
sys.path.insert(0, "/home/barberb/ipfs_datasets_py")

# Bad: relative imports
import validate_collection_completeness
from cc_domain_parquet_locator import find_domain_files

def main():
    validator = validate_collection_completeness.CollectionValidator()
    files = find_domain_files("example.com")
    return 0

if __name__ == "__main__":
    sys.exit(main())
```

**After Migration (âœ…):**
```python
#!/usr/bin/env python3
"""Canonical implementation in src/municipal_scrape_workspace/ccindex/"""

# Good: Package imports
from municipal_scrape_workspace.ccindex.validate_collection_completeness import CollectionValidator
from municipal_scrape_workspace.ccindex.cc_domain_parquet_locator import find_domain_files

def main(argv=None) -> int:
    """Main entry point."""
    import argparse
    parser = argparse.ArgumentParser(description="...")
    args = parser.parse_args(argv)
    
    validator = CollectionValidator()
    files = find_domain_files("example.com")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())
```

**Root Wrapper (âœ…):**
```python
#!/usr/bin/env python3
"""Backwards-compatible wrapper.

Moved to:
  municipal_scrape_workspace.ccindex.my_tool
"""

from municipal_scrape_workspace.ccindex.my_tool import main

if __name__ == "__main__":
    raise SystemExit(main())
```

---

## ğŸš€ Running Tools

### Method 1: Via Root Wrappers (Backwards Compatible)

This method works exactly like before the migration:

```bash
# Activate environment
source .venv/bin/activate

# Run tools directly (wrappers delegate to src/)
./search_cc_domain.py --domain example.com
./build_cc_pointer_duckdb.py --help
./cc_pipeline_orchestrator.py --config pipeline_config.json
```

**Pros:**
- Works exactly like before migration
- No need to change existing scripts
- Familiar to users

**Cons:**
- Requires being in repository root
- Doesn't work from installed package elsewhere

### Method 2: Via Python Module (New, Recommended)

Run tools as Python modules from anywhere:

```bash
# Run as module
python -m municipal_scrape_workspace.ccindex.search_cc_domain --domain example.com
python -m municipal_scrape_workspace.ccindex.build_cc_pointer_duckdb --help
python -m municipal_scrape_workspace.ccindex.cc_pipeline_orchestrator --config pipeline_config.json
```

**Pros:**
- Works from anywhere after `pip install`
- Clean, unambiguous
- Standard Python practice

**Cons:**
- Longer command

### Method 3: Via Console Scripts (If Configured)

If console script entry points are added to `pyproject.toml`:

```bash
# Run via installed console script
ccindex-search --domain example.com
ccindex-build-pointer --help
ccindex-orchestrate --config pipeline_config.json
```

**Configuration needed in `pyproject.toml`:**
```toml
[project.scripts]
municipal-scrape = "municipal_scrape_workspace.cli:main"
ccindex-search = "municipal_scrape_workspace.ccindex.search_cc_via_meta_indexes:main"
ccindex-build-pointer = "municipal_scrape_workspace.ccindex.build_cc_pointer_duckdb:main"
ccindex-orchestrate = "municipal_scrape_workspace.ccindex.cc_pipeline_orchestrator:main"
```

**Pros:**
- Shortest commands
- Most user-friendly
- Works system-wide after install

**Cons:**
- Requires explicit configuration
- Need to choose which tools get shortcuts

### Method 4: Shell Scripts

Operational scripts remain in `scripts/ops/`:

```bash
# Via canonical location
./scripts/ops/overnight_build_duckdb_index.sh

# Via root wrapper (backwards compatible)
./overnight_build_duckdb_index.sh
```

---

## ğŸ“¦ Dependency Management

### Installing the Package

#### Basic Installation (Municipal Scrape Only)

```bash
# Clone repository
git clone https://github.com/endomorphosis/municipal_scrape_workspace.git
cd municipal_scrape_workspace

# Install basic package
./bootstrap.sh
source .venv/bin/activate
```

#### With Common Crawl Tools

```bash
# Install with CC index tooling
pip install -e '.[ccindex]'
```

This installs:
- `duckdb>=0.10.0`
- `pyarrow>=14.0.0`
- `psutil>=5.9.0`
- `requests>=2.31.0`

#### With Optional Features

```bash
# Install with Playwright support
pip install -e '.[playwright]'

# Install everything
pip install -e '.[ccindex,playwright]'
```

### Dependency Structure

```toml
[project]
dependencies = [
    # Core dependencies (minimal)
    # ipfs_datasets_py would go here when resolved
]

[project.optional-dependencies]
ccindex = [
    # Heavy dependencies for CC tooling
    "duckdb>=0.10.0",
    "pyarrow>=14.0.0",
    "psutil>=5.9.0",
    "requests>=2.31.0",
]

playwright = [
    # Fallback scraping support
    "playwright>=1.45",
]
```

### Lazy Import Pattern

To allow `--help` without installing heavy dependencies:

```python
def main(argv=None) -> int:
    """Main entry point."""
    import argparse
    parser = argparse.ArgumentParser(description="Search CC indexes")
    parser.add_argument("--domain", required=True)
    args = parser.parse_args(argv)
    
    # Import heavy dependencies only when actually running
    import duckdb
    import pyarrow.parquet as pq
    
    # ... use dependencies
    return 0
```

---

## âš ï¸ Remaining Gaps

### 1. ipfs_datasets_py Dependency (High Priority)

**Current Issue:**
```toml
# In pyproject.toml - CURRENTLY COMMENTED OUT
# "ipfs_datasets_py @ file:///home/barberb/ipfs_datasets_py",
```

This hardcoded local path is not portable across environments.

**Recommended Solutions:**

**Option A: Git URL (Recommended)**
```toml
dependencies = [
    "ipfs_datasets_py @ git+https://github.com/endomorphosis/ipfs_datasets_py.git@main",
]
```

**Option B: Make Optional with Dev Override**
```toml
[project.optional-dependencies]
ipfs = [
    "ipfs_datasets_py @ git+https://github.com/endomorphosis/ipfs_datasets_py.git@main",
]
```

Then support local dev:
```bash
export IPFS_DATASETS_PY_ROOT="/path/to/local/ipfs_datasets_py"
pip install -e .
```

**Option C: Publish to PyPI (Best Long-term)**
```toml
dependencies = [
    "ipfs-datasets-py>=0.1.0",
]
```

### 2. Testing Infrastructure

**Current State:** No test suite exists

**Recommended Setup:**
```toml
[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "pytest-asyncio>=0.21",
    "black>=23.0",
    "ruff>=0.1.0",
    "mypy>=1.0",
]
```

**Basic Test Structure:**
```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_ccindex/
â”‚   â”œâ”€â”€ test_search_domain.py
â”‚   â”œâ”€â”€ test_build_index.py
â”‚   â””â”€â”€ test_validation.py
â””â”€â”€ test_municipal_scrape/
    â””â”€â”€ test_orchestrator.py
```

### 3. Documentation Gaps

**Needed:**
- [ ] API documentation for each module
- [ ] Usage examples for common workflows
- [ ] Troubleshooting guide
- [ ] Performance tuning guide
- [ ] Contributing guidelines

### 4. Console Script Entry Points

**Current State:** Only `municipal-scrape` CLI defined

**Recommended Additions:**
```toml
[project.scripts]
municipal-scrape = "municipal_scrape_workspace.cli:main"
ccindex-search = "municipal_scrape_workspace.ccindex.search_cc_via_meta_indexes:main"
ccindex-build = "municipal_scrape_workspace.ccindex.build_cc_pointer_duckdb:main"
ccindex-orchestrate = "municipal_scrape_workspace.ccindex.cc_pipeline_orchestrator:main"
```

### 5. Configuration Management

**Current State:** Config files scattered

**Recommended:** Centralize configuration
```
config/
â”œâ”€â”€ default.json
â”œâ”€â”€ pipeline_config.json
â””â”€â”€ development.json
```

---

## ğŸ› ï¸ Development Workflow

### Setting Up Development Environment

```bash
# 1. Clone repository
git clone https://github.com/endomorphosis/municipal_scrape_workspace.git
cd municipal_scrape_workspace

# 2. Run bootstrap script
./bootstrap.sh

# 3. Activate virtual environment
source .venv/bin/activate

# 4. Install with development dependencies
pip install -e '.[ccindex,dev]'  # When [dev] is added

# 5. Verify installation
municipal-scrape --help
python -m municipal_scrape_workspace.ccindex.search_cc_domain --help
```

### Making Changes to Code

#### Editing a ccindex Tool

```bash
# 1. Edit canonical implementation
vim src/municipal_scrape_workspace/ccindex/search_cc_domain.py

# 2. Test via wrapper (no reinstall needed with -e)
./search_cc_domain.py --domain example.com

# 3. Test via module
python -m municipal_scrape_workspace.ccindex.search_cc_domain --domain example.com

# 4. Changes are immediately reflected (editable install)
```

#### Adding a New Tool

```bash
# 1. Create canonical implementation
vim src/municipal_scrape_workspace/ccindex/my_new_tool.py

# 2. Ensure it has main() function
def main(argv=None) -> int:
    """Main entry point."""
    # Implementation
    return 0

if __name__ == "__main__":
    raise SystemExit(main())

# 3. Create root wrapper
cat > my_new_tool.py << 'EOF'
#!/usr/bin/env python3
"""Backwards-compatible wrapper for My New Tool.

Moved to:
  municipal_scrape_workspace.ccindex.my_new_tool
"""

from municipal_scrape_workspace.ccindex.my_new_tool import main

if __name__ == "__main__":
    raise SystemExit(main())
EOF

# 4. Make executable
chmod +x my_new_tool.py

# 5. Test both methods
./my_new_tool.py --help
python -m municipal_scrape_workspace.ccindex.my_new_tool --help
```

### Running Tests (When Implemented)

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/test_ccindex/test_search_domain.py

# Run with coverage
pytest --cov=municipal_scrape_workspace
```

### Code Quality Checks (When Configured)

```bash
# Format code
black src/

# Lint code
ruff check src/

# Type checking
mypy src/
```

---

## ğŸ“š Related Documentation

### Refactoring Documentation
- [REFACTORING_INDEX.md](REFACTORING_INDEX.md) - Complete refactoring documentation index
- [REFACTORING_ROADMAP.md](REFACTORING_ROADMAP.md) - Original refactoring plan
- [REFACTORING_STATUS.md](REFACTORING_STATUS.md) - Migration progress tracking
- [FILE_MIGRATION_MAP.md](FILE_MIGRATION_MAP.md) - File-by-file migration mapping
- [REFACTORING_CHECKLIST.md](REFACTORING_CHECKLIST.md) - Execution checklist

### Repository Documentation
- [README.md](README.md) - Project overview
- [REPO_LAYOUT_README.md](REPO_LAYOUT_README.md) - Original layout conventions
- [docs/REPO_LAYOUT_PLAN.md](docs/REPO_LAYOUT_PLAN.md) - Detailed layout plan

### Domain Documentation
- [INDEX_ARCHITECTURE.md](INDEX_ARCHITECTURE.md) - CC index architecture
- [DUCKDB_INDEX_DESIGN.md](DUCKDB_INDEX_DESIGN.md) - DuckDB index design
- [docs/COMMON_CRAWL_USAGE.md](docs/COMMON_CRAWL_USAGE.md) - CC usage guide

---

## âœ… Migration Verification Checklist

Use this to verify the migration is complete:

- [x] All Python files in root are thin wrappers (â‰¤15 lines)
- [x] All canonical implementations in `src/municipal_scrape_workspace/`
- [x] All superseded files in `archive/ccindex/superseded/`
- [x] All files use package imports (no sys.path hacks)
- [x] All wrappers have correct shebang and imports
- [x] Package installs cleanly with `pip install -e .`
- [x] Package installs with extras `pip install -e '.[ccindex]'`
- [ ] All wrappers execute correctly `./tool.py --help`
- [ ] All modules execute correctly `python -m module --help`
- [x] Documentation reflects new structure
- [ ] Tests pass (when implemented)
- [ ] ipfs_datasets_py dependency resolved

---

## ğŸ¯ Success Criteria

The migration is considered complete when:

1. âœ… **Code Organization**: All files in proper locations
2. âœ… **Clean Imports**: No sys.path hacks, only package imports
3. âœ… **Backwards Compatible**: All old scripts still work via wrappers
4. âœ… **Installable**: Package installs via pip
5. âœ… **Documented**: Comprehensive documentation of new structure
6. âš ï¸ **Portable Dependencies**: ipfs_datasets_py dependency resolved
7. â³ **Tested**: Test suite implemented (future work)
8. â³ **CI/CD**: Automated testing configured (future work)

---

## ğŸ“ Getting Help

If you have questions about the new structure:

1. **Quick lookup**: Check [FILE_MIGRATION_MAP.md](FILE_MIGRATION_MAP.md)
2. **Import patterns**: See [Import Patterns](#import-patterns) section above
3. **Running tools**: See [Running Tools](#running-tools) section above
4. **Dependencies**: See [Dependency Management](#dependency-management) section above

---

**Last Updated**: 2026-01-19  
**Migration Status**: âœ… COMPLETE  
**Next Steps**: Resolve ipfs_datasets_py dependency, implement testing
