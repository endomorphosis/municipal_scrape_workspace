# Repository Refactoring Roadmap

**Status**: Analysis Complete  
**Last Updated**: 2026-01-19  
**Purpose**: Document the complete file migration plan, import refactoring requirements, and dependency gaps

---

## Table of Contents

1. [Current State Analysis](#current-state-analysis)
2. [Final Repository Structure](#final-repository-structure)
3. [File Migration Status](#file-migration-status)
4. [Files Requiring Action](#files-requiring-action)
5. [Import Refactoring Guidelines](#import-refactoring-guidelines)
6. [Dependency Gaps](#dependency-gaps)
7. [Running Tools After Migration](#running-tools-after-migration)

---

## Current State Analysis

### Repository Overview

This repository contains two major components:

1. **Municipal Scraping Workflow** - An installable Python package for scraping municipal websites
2. **Common Crawl (CC) Index Pipeline** - Tools for building and querying DuckDB/Parquet indexes of Common Crawl data

### Current File Distribution

```
Root directory:        52 Python files
  - Wrappers:          19 files (already migrated)
  - Full impl:         33 files (need migration or archival)
  
src/ccindex:          23 canonical implementations
benchmarks/ccindex:   10 benchmark scripts
scripts/ops:          Multiple shell scripts
archive/ccindex:      5 archived converters
```

### Files Already Migrated to `src/municipal_scrape_workspace/ccindex/`

These files have been successfully migrated with root-level wrappers in place:

1. `build_cc_pointer_duckdb.py` âœ“
2. `build_index_from_parquet.py` âœ“
3. `build_master_index.py` âœ“
4. `build_parallel_duckdb_indexes.py` âœ“
5. `build_year_meta_indexes.py` âœ“
6. `cc_domain_parquet_locator.py` âœ“
7. `download_warc_records.py` âœ“
8. `parallel_validate_parquet.py` âœ“
9. `search_cc_domain.py` âœ“
10. `search_cc_duckdb_index.py` âœ“
11. `search_cc_pointer_index.py` âœ“
12. `search_cc_via_meta_indexes.py` âœ“
13. `search_parallel_duckdb_indexes.py` âœ“
14. `sort_cc_parquet_shards.py` âœ“
15. `validate_and_sort_parquet.py` âœ“
16. `validate_collection_completeness.py` âœ“
17. `validate_warc_record_blobs.py` âœ“
18. `verify_warc_retrieval.py` âœ“
19. `warc_candidates_from_jsonl.py` âœ“

---

## Final Repository Structure

```
.
â”œâ”€â”€ src/municipal_scrape_workspace/          # Installable Python package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py                               # Main CLI entrypoint
â”‚   â”œâ”€â”€ orchestrate_municipal_scrape.py      # Municipal scrape orchestrator
â”‚   â”œâ”€â”€ check_archive_callbacks.py           # Archive integration checks
â”‚   â”‚
â”‚   â””â”€â”€ ccindex/                             # Common Crawl tooling
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”‚
â”‚       â”œâ”€â”€ # Core pipeline orchestration
â”‚       â”œâ”€â”€ cc_pipeline_orchestrator.py
â”‚       â”œâ”€â”€ cc_pipeline_watch.py
â”‚       â”œâ”€â”€ cc_pipeline_hud.py
â”‚       â”œâ”€â”€ monitor_progress.py
â”‚       â”‚
â”‚       â”œâ”€â”€ # Conversion tools
â”‚       â”œâ”€â”€ bulk_convert_gz_to_parquet.py
â”‚       â”œâ”€â”€ parallel_convert_missing.py
â”‚       â”œâ”€â”€ regenerate_parquet_from_gz.py
â”‚       â”œâ”€â”€ sample_ccindex_to_parquet.py
â”‚       â”œâ”€â”€ extract_cc_index_tarballs.py
â”‚       â”‚
â”‚       â”œâ”€â”€ # Sorting tools (keep canonical only)
â”‚       â”œâ”€â”€ sort_cc_parquet_shards.py        [DONE]
â”‚       â”œâ”€â”€ sort_unsorted_memory_aware.py    [CANONICAL]
â”‚       â”‚
â”‚       â”œâ”€â”€ # Validation tools
â”‚       â”œâ”€â”€ validate_and_sort_parquet.py     [DONE]
â”‚       â”œâ”€â”€ parallel_validate_parquet.py     [DONE]
â”‚       â”œâ”€â”€ validate_urlindex_sorted.py
â”‚       â”œâ”€â”€ validate_search_completeness.py
â”‚       â”œâ”€â”€ validate_collection_completeness.py [DONE]
â”‚       â”‚
â”‚       â”œâ”€â”€ # Index builders
â”‚       â”œâ”€â”€ build_cc_pointer_duckdb.py       [DONE]
â”‚       â”œâ”€â”€ build_index_from_parquet.py      [DONE]
â”‚       â”œâ”€â”€ build_parallel_duckdb_indexes.py [DONE]
â”‚       â”œâ”€â”€ build_duckdb_pointer_from_parquet.py
â”‚       â”œâ”€â”€ build_cc_parquet_rowgroup_index.py
â”‚       â”‚
â”‚       â”œâ”€â”€ # Meta-index tools
â”‚       â”œâ”€â”€ build_year_meta_indexes.py       [DONE]
â”‚       â”œâ”€â”€ build_master_index.py            [DONE]
â”‚       â”‚
â”‚       â”œâ”€â”€ # Search tools
â”‚       â”œâ”€â”€ search_cc_domain.py              [DONE]
â”‚       â”œâ”€â”€ search_cc_duckdb_index.py        [DONE]
â”‚       â”œâ”€â”€ search_cc_pointer_index.py       [DONE]
â”‚       â”œâ”€â”€ search_cc_via_meta_indexes.py    [DONE]
â”‚       â”œâ”€â”€ search_parallel_duckdb_indexes.py [DONE]
â”‚       â”œâ”€â”€ cc_domain_parquet_locator.py     [DONE]
â”‚       â”‚
â”‚       â”œâ”€â”€ # WARC retrieval tools
â”‚       â”œâ”€â”€ download_warc_records.py         [DONE]
â”‚       â”œâ”€â”€ verify_warc_retrieval.py         [DONE]
â”‚       â”œâ”€â”€ validate_warc_record_blobs.py    [DONE]
â”‚       â”œâ”€â”€ warc_candidates_from_jsonl.py    [DONE]
â”‚       â”‚
â”‚       â””â”€â”€ # Monitoring/queue tools
â”‚           â”œâ”€â”€ queue_cc_pointer_build.py
â”‚           â”œâ”€â”€ launch_cc_pointer_build.py
â”‚           â”œâ”€â”€ monitor_cc_pointer_build.py
â”‚           â”œâ”€â”€ watchdog_cc_pointer_build.py
â”‚           â”œâ”€â”€ watchdog_monitor.py
â”‚           â””â”€â”€ cc_pointer_status.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ops/                                 # Operational shell scripts
â”‚       â”œâ”€â”€ download_cc_indexes*.sh
â”‚       â”œâ”€â”€ overnight_*.sh
â”‚       â”œâ”€â”€ monitor_*.sh
â”‚       â”œâ”€â”€ rebuild_*.sh
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ ccindex/                             # Performance benchmarks
â”‚       â”œâ”€â”€ benchmark_*.py
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ archive/
â”‚   â””â”€â”€ ccindex/
â”‚       â”œâ”€â”€ converters/                      # One-off conversion scripts
â”‚       â”‚   â”œâ”€â”€ convert_final_three*.py
â”‚       â”‚   â””â”€â”€ convert_missing*.py
â”‚       â””â”€â”€ superseded/                      # Deprecated tools
â”‚           â”œâ”€â”€ cc_pipeline_manager.py       [TO BE ARCHIVED]
â”‚           â”œâ”€â”€ consolidate_parquet_files.py [TO BE ARCHIVED]
â”‚           â”œâ”€â”€ sort_unsorted_files.py       [TO BE ARCHIVED]
â”‚           â”œâ”€â”€ sort_parquet_external_merge.py [TO BE ARCHIVED]
â”‚           â”œâ”€â”€ validate_and_mark_sorted.py  [TO BE ARCHIVED]
â”‚           â”œâ”€â”€ build_duckdb_from_sorted_parquet.py [TO BE ARCHIVED]
â”‚           â””â”€â”€ compare_crawl_results.py     [TO BE ARCHIVED]
â”‚
â”œâ”€â”€ docs/                                    # Documentation
â”‚   â”œâ”€â”€ REPO_LAYOUT_PLAN.md
â”‚   â”œâ”€â”€ COMMON_CRAWL_USAGE.md
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ <root-level wrappers>                    # Backwards compatibility
    â”œâ”€â”€ search_cc_domain.py                  # Thin wrapper imports from src/
    â”œâ”€â”€ build_cc_pointer_duckdb.py           # Thin wrapper imports from src/
    â””â”€â”€ ...
```

---

## File Migration Status

### Category 1: Already Migrated (19 files)

These files exist in `src/municipal_scrape_workspace/ccindex/` with thin wrappers at root:

- âœ… build_cc_pointer_duckdb.py
- âœ… build_index_from_parquet.py
- âœ… build_master_index.py
- âœ… build_parallel_duckdb_indexes.py
- âœ… build_year_meta_indexes.py
- âœ… cc_domain_parquet_locator.py
- âœ… download_warc_records.py
- âœ… parallel_validate_parquet.py
- âœ… search_cc_domain.py
- âœ… search_cc_duckdb_index.py
- âœ… search_cc_pointer_index.py
- âœ… search_cc_via_meta_indexes.py
- âœ… search_parallel_duckdb_indexes.py
- âœ… sort_cc_parquet_shards.py
- âœ… validate_and_sort_parquet.py
- âœ… validate_collection_completeness.py
- âœ… validate_warc_record_blobs.py
- âœ… verify_warc_retrieval.py
- âœ… warc_candidates_from_jsonl.py

### Category 2: Migrated to src/ But Missing Wrapper (4 files)

These exist in both locations but root file is not a wrapper:

- âš ï¸ `build_cc_parquet_rowgroup_index.py` - Root file needs to become wrapper
- âš ï¸ `bulk_convert_gz_to_parquet.py` - Root file needs to become wrapper
- âš ï¸ `validate_search_completeness.py` - Root file needs to become wrapper  
- âš ï¸ `validate_urlindex_sorted.py` - Root file needs to become wrapper

**Action Required**: Convert root files to wrappers that import from src/

### Category 3: Need Migration to src/ (17 files)

These are full implementations that should be moved to `src/municipal_scrape_workspace/ccindex/`:

#### Orchestration/Monitoring (6 files)
- ğŸ“¦ `cc_pipeline_orchestrator.py` â†’ `src/.../ccindex/cc_pipeline_orchestrator.py`
- ğŸ“¦ `cc_pipeline_watch.py` â†’ `src/.../ccindex/cc_pipeline_watch.py`
- ğŸ“¦ `cc_pipeline_hud.py` â†’ `src/.../ccindex/cc_pipeline_hud.py`
- ğŸ“¦ `monitor_progress.py` â†’ `src/.../ccindex/monitor_progress.py`
- ğŸ“¦ `monitor_cc_pointer_build.py` â†’ `src/.../ccindex/monitor_cc_pointer_build.py`
- ğŸ“¦ `cc_pointer_status.py` â†’ `src/.../ccindex/cc_pointer_status.py`

#### Pointer Build Queue/Watchdog (3 files)
- ğŸ“¦ `queue_cc_pointer_build.py` â†’ `src/.../ccindex/queue_cc_pointer_build.py`
- ğŸ“¦ `launch_cc_pointer_build.py` â†’ `src/.../ccindex/launch_cc_pointer_build.py`
- ğŸ“¦ `watchdog_cc_pointer_build.py` â†’ `src/.../ccindex/watchdog_cc_pointer_build.py`
- ğŸ“¦ `watchdog_monitor.py` â†’ `src/.../ccindex/watchdog_monitor.py`

#### Conversion Tools (4 files)
- ğŸ“¦ `parallel_convert_missing.py` â†’ `src/.../ccindex/parallel_convert_missing.py`
- ğŸ“¦ `regenerate_parquet_from_gz.py` â†’ `src/.../ccindex/regenerate_parquet_from_gz.py`
- ğŸ“¦ `sample_ccindex_to_parquet.py` â†’ `src/.../ccindex/sample_ccindex_to_parquet.py`
- ğŸ“¦ `extract_cc_index_tarballs.py` â†’ `src/.../ccindex/extract_cc_index_tarballs.py`

#### Index Builders (1 file)
- ğŸ“¦ `build_duckdb_pointer_from_parquet.py` â†’ `src/.../ccindex/build_duckdb_pointer_from_parquet.py`

#### Municipal Scrape (2 files)
- ğŸ“¦ `orchestrate_municipal_scrape.py` â†’ `src/municipal_scrape_workspace/orchestrate_municipal_scrape.py`
- ğŸ“¦ `check_archive_callbacks.py` â†’ `src/municipal_scrape_workspace/check_archive_callbacks.py`

### Category 4: Archive as Duplicate/Superseded (7 files)

These should be moved to `archive/ccindex/superseded/`:

#### Superseded by Orchestrator
- ğŸ—„ï¸ `cc_pipeline_manager.py` â†’ `archive/ccindex/superseded/`  
  *Reason: Superseded by cc_pipeline_orchestrator.py*

#### Duplicate/Redundant Search Tools (4 files)
- ğŸ—„ï¸ `search_domain_duckdb_pointer.py` â†’ `archive/ccindex/superseded/`  
  *Reason: Duplicate of search_parallel_duckdb_indexes.py*
- ğŸ—„ï¸ `search_domain_pointer_index.py` â†’ `archive/ccindex/superseded/`  
  *Reason: Duplicate functionality*
- ğŸ—„ï¸ `search_duckdb_domain.py` â†’ `archive/ccindex/superseded/`  
  *Reason: Covered by canonical search tools*
- ğŸ—„ï¸ `search_duckdb_pointer_domain.py` â†’ `archive/ccindex/superseded/`  
  *Reason: Covered by canonical search tools*

#### Superseded Sort/Validate Tools (2 files)
- ğŸ—„ï¸ `sort_unsorted_files.py` â†’ `archive/ccindex/superseded/`  
  *Reason: Keep sort_unsorted_memory_aware.py as canonical*
- ğŸ—„ï¸ `sort_parquet_external_merge.py` â†’ `archive/ccindex/superseded/`  
  *Reason: Functionality covered by canonical sorters*

### Category 5: Evaluate and Decide (5 files)

These need case-by-case evaluation:

- â“ `consolidate_parquet_files.py` - Keep if actively used, else archive
- â“ `compare_crawl_results.py` - Archive if one-off, keep if reusable utility
- â“ `validate_and_mark_sorted.py` - Evaluate vs validate_and_sort_parquet.py
- â“ `build_duckdb_from_sorted_parquet.py` - Evaluate vs build_duckdb_pointer_from_parquet.py
- â“ `sort_unsorted_memory_aware.py` - Keep as canonical memory-aware sorter

---

## Files Requiring Action

### Immediate Actions

#### 1. Fix Missing Wrappers (4 files)

These files are in `src/` but root version is not a wrapper:

```bash
# For each file, convert root version to thin wrapper:

# build_cc_parquet_rowgroup_index.py
# bulk_convert_gz_to_parquet.py
# validate_search_completeness.py
# validate_urlindex_sorted.py
```

**Wrapper Template**:
```python
#!/usr/bin/env python3
"""Backwards-compatible wrapper for <tool name>.

Moved to:
  municipal_scrape_workspace.ccindex.<module_name>
"""

from municipal_scrape_workspace.ccindex.<module_name> import main

if __name__ == "__main__":
    raise SystemExit(main())
```

#### 2. Migrate High-Priority Files (10 files)

Move these core tools to `src/municipal_scrape_workspace/ccindex/`:

**Orchestration** (Priority 1):
- cc_pipeline_orchestrator.py
- cc_pipeline_watch.py  
- cc_pipeline_hud.py
- monitor_progress.py

**Queue/Watchdog** (Priority 2):
- queue_cc_pointer_build.py
- launch_cc_pointer_build.py
- monitor_cc_pointer_build.py
- watchdog_cc_pointer_build.py
- watchdog_monitor.py
- cc_pointer_status.py

**Municipal Scrape** (Priority 3):
- orchestrate_municipal_scrape.py
- check_archive_callbacks.py

#### 3. Archive Superseded Files (7 files)

Move to `archive/ccindex/superseded/`:
- cc_pipeline_manager.py
- search_domain_duckdb_pointer.py
- search_domain_pointer_index.py
- search_duckdb_domain.py
- search_duckdb_pointer_domain.py
- sort_unsorted_files.py
- sort_parquet_external_merge.py

---

## Import Refactoring Guidelines

### When Moving a File to src/

1. **Add/Preserve main() Function**
   ```python
   def main(argv=None) -> int:
       """Main entry point."""
       parser = argparse.ArgumentParser(...)
       args = parser.parse_args(argv)
       # ... implementation
       return 0  # or exit code
   ```

2. **Replace Intra-repo Imports**
   
   âŒ **Before** (root-level import):
   ```python
   import validate_collection_completeness
   from cc_domain_parquet_locator import find_domain_files
   ```
   
   âœ… **After** (package import):
   ```python
   from municipal_scrape_workspace.ccindex.validate_collection_completeness import CollectionValidator
   from municipal_scrape_workspace.ccindex.cc_domain_parquet_locator import find_domain_files
   ```

3. **Remove sys.path Hacks**
   
   âŒ **Remove**:
   ```python
   sys.path.insert(0, str(Path(__file__).parent))
   sys.path.insert(0, "/home/barberb/ipfs_datasets_py")
   ```

4. **Lazy Import Heavy Dependencies**
   
   For optional dependencies (allows `--help` without installing ccindex extras):
   ```python
   def main(argv=None) -> int:
       import duckdb  # Import here, not at module level
       import pyarrow.parquet as pq
       # ... use dependencies
   ```

5. **Update Documentation References**
   
   Update any docstrings, comments, or docs that reference file locations:
   ```python
   """
   Search CC indexes via meta-indexes.
   
   Canonical location:
     src/municipal_scrape_workspace/ccindex/search_cc_via_meta_indexes.py
   
   Run via wrapper:
     ./search_cc_via_meta_indexes.py --help
   
   Run via module:
     python -m municipal_scrape_workspace.ccindex.search_cc_via_meta_indexes --help
   """
   ```

### Creating Backwards-Compatible Wrappers

For every file moved to `src/`, create a thin wrapper at the root:

```python
#!/usr/bin/env python3
"""Backwards-compatible wrapper for <Tool Name>.

Moved to:
  municipal_scrape_workspace.ccindex.<module_name>
"""

from municipal_scrape_workspace.ccindex.<module_name> import main

if __name__ == "__main__":
    raise SystemExit(main())
```

**Key Points**:
- Keep the wrapper minimal (no business logic)
- Preserve the original filename at root
- Use `raise SystemExit(main())` to propagate exit codes correctly

### Import Pattern Examples

#### Example 1: Orchestrator Imports Validator

**File**: `cc_pipeline_orchestrator.py`

âŒ **Before**:
```python
import validate_collection_completeness
validator = validate_collection_completeness.CollectionValidator(...)
```

âœ… **After**:
```python
from municipal_scrape_workspace.ccindex.validate_collection_completeness import CollectionValidator
validator = CollectionValidator(...)
```

#### Example 2: Municipal Scrape Calls ipfs_datasets_py

**File**: `orchestrate_municipal_scrape.py`

âŒ **Before** (hardcoded path):
```python
sys.path.insert(0, "/home/barberb/ipfs_datasets_py")
from ipfs_datasets.unified_scraper import UnifiedScraper
```

âœ… **After** (installed dependency):
```python
# Just import - ipfs_datasets_py is in pyproject.toml dependencies
from ipfs_datasets.unified_scraper import UnifiedScraper
```

Or with dev override:
```python
import os
import sys

# Support local dev checkout via environment variable
ipfs_root = os.environ.get("IPFS_DATASETS_PY_ROOT")
if ipfs_root:
    sys.path.insert(0, ipfs_root)

from ipfs_datasets.unified_scraper import UnifiedScraper
```

---

## Dependency Gaps

### 1. ipfs_datasets_py Portability Issue

**Current Problem**:
```toml
dependencies = [
    "ipfs_datasets_py @ file:///home/barberb/ipfs_datasets_py",
]
```

This hardcoded local path is not portable across development environments.

**Solutions**:

**Option A: Git URL Dependency** (Recommended if upstream is stable)
```toml
dependencies = [
    "ipfs_datasets_py @ git+https://github.com/endomorphosis/ipfs_datasets_py.git@main",
]
```

**Option B: Make it Optional with Dev Override**
```toml
[project.optional-dependencies]
ipfs = [
    "ipfs_datasets_py @ git+https://github.com/endomorphosis/ipfs_datasets_py.git@main",
]
```

Then document the dev override pattern:
```bash
# For local development
export IPFS_DATASETS_PY_ROOT="/path/to/local/ipfs_datasets_py"
pip install -e .

# For production
pip install -e '.[ipfs]'
```

**Option C: Published Package** (Best long-term)
```toml
dependencies = [
    "ipfs-datasets-py>=0.1.0",  # If/when published to PyPI
]
```

### 2. Optional CCIndex Dependencies

**Current State**: âœ… Already handled correctly

The `[ccindex]` optional extra properly declares heavy dependencies:

```toml
[project.optional-dependencies]
ccindex = [
  "duckdb>=0.10.0",
  "pyarrow>=14.0.0",
  "psutil>=5.9.0",
  "requests>=2.31.0",
]
```

**Usage**:
```bash
# Install with CC tooling
pip install -e '.[ccindex]'

# Install without CC tooling (lighter)
pip install -e .
```

### 3. Development Dependencies

**Missing**: No dev/test extras currently defined

**Recommendation**: Add development dependencies
```toml
[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "pytest-asyncio>=0.21",
    "black>=23.0",
    "ruff>=0.1.0",
    "mypy>=1.0",
]
```

---

## Running Tools After Migration

### Via Root Wrapper (Backwards Compatible)

```bash
# Activate virtual environment
source .venv/bin/activate

# Run via wrapper (old way still works)
./search_cc_domain.py --domain example.com
./build_cc_pointer_duckdb.py --help
```

### Via Python Module (New Way)

```bash
# Run as module
python -m municipal_scrape_workspace.ccindex.search_cc_domain --domain example.com
python -m municipal_scrape_workspace.ccindex.build_cc_pointer_duckdb --help
```

### Via Console Script Entry Points (Future Enhancement)

Add to `pyproject.toml`:
```toml
[project.scripts]
municipal-scrape = "municipal_scrape_workspace.cli:main"

# Optional: Add ccindex tool entry points
ccindex-search = "municipal_scrape_workspace.ccindex.search_cc_via_meta_indexes:main"
ccindex-build-pointer = "municipal_scrape_workspace.ccindex.build_cc_pointer_duckdb:main"
ccindex-orchestrate = "municipal_scrape_workspace.ccindex.cc_pipeline_orchestrator:main"
```

Then run directly:
```bash
ccindex-search --domain example.com
ccindex-build-pointer --help
```

### Shell Scripts

```bash
# Via ops scripts (canonical location)
./scripts/ops/overnight_build_duckdb_index.sh

# Via root wrapper (backwards compatible)
./overnight_build_duckdb_index.sh
```

---

## Migration Checklist

### For Each File Being Migrated:

- [ ] 1. Move file to appropriate location in `src/municipal_scrape_workspace/ccindex/`
- [ ] 2. Add/verify `main(argv=None) -> int` function exists
- [ ] 3. Update all intra-repo imports to use package imports
- [ ] 4. Remove any `sys.path.insert()` hacks
- [ ] 5. Add lazy imports for optional dependencies if needed
- [ ] 6. Update docstring with canonical location
- [ ] 7. Create thin wrapper at root with original filename
- [ ] 8. Test wrapper works: `./tool.py --help`
- [ ] 9. Test module import works: `python -m municipal_scrape_workspace.ccindex.tool --help`
- [ ] 10. Update any documentation referencing the old location
- [ ] 11. Check for any scripts/docs that reference this file and update them

### For Files Being Archived:

- [ ] 1. Move to `archive/ccindex/superseded/`
- [ ] 2. Add README in archive explaining why archived
- [ ] 3. Update any docs that reference the archived file
- [ ] 4. Note canonical replacement tool (if applicable)

### Final Validation:

- [ ] 1. All root `.py` files are either wrappers or archived
- [ ] 2. All imports use package imports (no relative/sys.path hacks)
- [ ] 3. `pip install -e .` works without ccindex dependencies
- [ ] 4. `pip install -e '.[ccindex]'` enables all CC tools
- [ ] 5. All wrappers execute correctly
- [ ] 6. Documentation reflects new structure
- [ ] 7. `.gitignore` excludes generated files (build artifacts, etc.)

---

## Summary of Changes Required

### Immediate (4 files)
- Convert 4 root files to wrappers (already in src/)

### High Priority (17 files)  
- Migrate 17 core tools to src/municipal_scrape_workspace/ccindex/
- Create wrappers for each

### Medium Priority (7 files)
- Archive 7 superseded/duplicate files

### Low Priority (5 files)
- Evaluate and decide on 5 ambiguous files

### Documentation
- âœ… REFACTORING_ROADMAP.md created (this document)
- Update REPO_LAYOUT_README.md with final state
- Update README.md with new structure

### Total Files to Process: 33 root Python files
